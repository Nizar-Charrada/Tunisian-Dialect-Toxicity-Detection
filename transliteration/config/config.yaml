# Config file for transliteration with simple transformer model

seed : 1 # random seed
source: arabic # source language
target: arabizi # target language

PAD_token : 0
SOS_token : 1
EOS_token : 2

trainer:
  max_epochs: 40 # number of epochs
  max_steps: null # precedence over max_epochs
  accumulate_grad_batches: 1 # accumulates grads every k batches
  gradient_clip_val: 1.0 # 0 means don't clip.
  row_log_interval: 50  # Interval of logging.
  test_size: 0.1 # test size

transformer:
  src_vocab_size : 13000
  tgt_vocab_size : 13000
  hidden_dim : 128
  encoder_layers: 1
  decoder_layers: 1
  dropout: 0.15
  nheads: 4

dataset:
  batch_size: 64
  shuffle: true
  num_samples: -1 # number of samples to be considered, -1 means all the dataset

optim:
  name: noam # optimizer implemented in transformer paper
  lr: 0.0
  factor: 1.0
  warmup: 4000


output_dir: transliteration/out

# Other parameters
load_model: False



